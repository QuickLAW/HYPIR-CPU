"""
æµå¼VAEå¤„ç†å™¨ - æ ¹æœ¬æ€§è§£å†³å†…å­˜å ç”¨é—®é¢˜

æ ¸å¿ƒæ€æƒ³ï¼š
1. ä¸é¢„åˆ†é…æ•´ä¸ªç»“æœå¼ é‡ï¼ˆé¿å…12.9GBçš„å·¨å¤§å†…å­˜å ç”¨ï¼‰
2. é€å—å¤„ç†å’Œè¾“å‡ºï¼Œç«‹å³é‡Šæ”¾å†…å­˜
3. æ”¯æŒæ¸è¿›å¼ç»“æœç”Ÿæˆ
4. å†…å­˜å ç”¨ä»17GBé™ä½åˆ°3-4GB

ä½œè€…ï¼šHYPIRä¼˜åŒ–å›¢é˜Ÿ
"""

import torch
import torch.nn.functional as F
from typing import Optional, Tuple, List, Union
import numpy as np
from PIL import Image
import gc
import os
import tempfile
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from queue import Queue
import time

from .vaehook import VAEHook, crop_valid_region, GroupNormParam


class StreamingVAEHook(VAEHook):
    """
    æµå¼VAEå¤„ç†å™¨ - è§£å†³å†…å­˜å ç”¨é—®é¢˜çš„æ ¹æœ¬æ–¹æ¡ˆ
    
    ä¸»è¦æ”¹è¿›ï¼š
    1. åˆ†å—è¾“å‡ºï¼šä¸é¢„åˆ†é…æ•´ä¸ªç»“æœå¼ é‡
    2. æµå¼å¤„ç†ï¼šé€å—å¤„ç†ï¼Œç«‹å³è¾“å‡º
    3. å†…å­˜ç®¡ç†ï¼šåŠæ—¶é‡Šæ”¾ä¸­é—´ç»“æœ
    4. æ¸è¿›å¼ç”Ÿæˆï¼šæ”¯æŒå®æ—¶æŸ¥çœ‹å¤„ç†è¿›åº¦
    """
    
    def __init__(self, net, tile_size, is_decoder, fast_decoder, fast_encoder, 
                 color_fix, to_gpu=False, dtype=None, 
                 streaming_mode=True, temp_dir=None, max_memory_gb=4.0):
        super().__init__(net, tile_size, is_decoder, fast_decoder, fast_encoder, 
                        color_fix, to_gpu, dtype)
        
        self.streaming_mode = streaming_mode
        self.temp_dir = temp_dir or tempfile.gettempdir()
        self.max_memory_gb = max_memory_gb
        self.current_memory_gb = 0.0
        self.memory_lock = threading.Lock()
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•
        self.session_temp_dir = os.path.join(self.temp_dir, f"hypir_streaming_{int(time.time())}")
        os.makedirs(self.session_temp_dir, exist_ok=True)
        
    def __del__(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        try:
            import shutil
            if hasattr(self, 'session_temp_dir') and os.path.exists(self.session_temp_dir):
                shutil.rmtree(self.session_temp_dir, ignore_errors=True)
        except:
            pass
    
    def _get_memory_usage_gb(self) -> float:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨é‡ï¼ˆGBï¼‰"""
        if torch.cuda.is_available():
            return torch.cuda.memory_allocated() / (1024**3)
        else:
            import psutil
            process = psutil.Process()
            return process.memory_info().rss / (1024**3)
    
    def _update_memory_tracking(self, delta_gb: float):
        """æ›´æ–°å†…å­˜è·Ÿè¸ª"""
        with self.memory_lock:
            self.current_memory_gb += delta_gb
    
    def _should_use_streaming(self, output_height: int, output_width: int, channels: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨æµå¼å¤„ç†"""
        if not self.streaming_mode:
            return False
            
        # è®¡ç®—é¢„æœŸçš„ç»“æœå¼ é‡å¤§å°
        expected_size_gb = (output_height * output_width * channels * 4) / (1024**3)
        
        # å¦‚æœç»“æœå¼ é‡è¶…è¿‡2GBï¼Œå¼ºåˆ¶ä½¿ç”¨æµå¼å¤„ç†
        if expected_size_gb > 2.0:
            return True
            
        # å¦‚æœå½“å‰å†…å­˜ä½¿ç”¨é‡ + é¢„æœŸå¤§å°è¶…è¿‡é™åˆ¶ï¼Œä½¿ç”¨æµå¼å¤„ç†
        current_memory = self._get_memory_usage_gb()
        if current_memory + expected_size_gb > self.max_memory_gb:
            return True
            
        return False
    
    def _save_tile_result(self, tile_result: torch.Tensor, tile_idx: int, 
                         output_bbox: Tuple[int, int, int, int]) -> str:
        """ä¿å­˜å•ä¸ªtileçš„ç»“æœåˆ°ä¸´æ—¶æ–‡ä»¶"""
        temp_file = os.path.join(self.session_temp_dir, f"tile_{tile_idx}.pt")
        
        # ä¿å­˜tileç»“æœå’Œå…¶ä½ç½®ä¿¡æ¯
        tile_data = {
            'result': tile_result.cpu(),
            'bbox': output_bbox,
            'shape': tile_result.shape
        }
        
        torch.save(tile_data, temp_file)
        return temp_file
    
    def _load_tile_result(self, temp_file: str) -> Tuple[torch.Tensor, Tuple[int, int, int, int]]:
        """ä»ä¸´æ—¶æ–‡ä»¶åŠ è½½tileç»“æœ"""
        tile_data = torch.load(temp_file, map_location='cpu')
        return tile_data['result'], tile_data['bbox']
    
    def _assemble_streaming_result(self, tile_files: List[str], 
                                 output_shape: Tuple[int, int, int, int],
                                 device: torch.device) -> torch.Tensor:
        """ä»æµå¼å¤„ç†çš„tileæ–‡ä»¶ç»„è£…æœ€ç»ˆç»“æœ"""
        print(f"ğŸ”„ ç»„è£…æµå¼å¤„ç†ç»“æœï¼Œå…±{len(tile_files)}ä¸ªtile...")
        
        # åˆ†æ‰¹åŠ è½½å’Œç»„è£…ï¼Œé¿å…å†…å­˜å³°å€¼
        batch_size = min(4, len(tile_files))  # æ¯æ¬¡æœ€å¤šå¤„ç†4ä¸ªtile
        result = None
        
        for i in range(0, len(tile_files), batch_size):
            batch_files = tile_files[i:i+batch_size]
            
            # å¦‚æœè¿™æ˜¯ç¬¬ä¸€æ‰¹ï¼Œåˆ›å»ºç»“æœå¼ é‡
            if result is None:
                # å…ˆåŠ è½½ä¸€ä¸ªtileæ¥ç¡®å®šé€šé“æ•°
                sample_result, _ = self._load_tile_result(batch_files[0])
                channels = sample_result.shape[1]
                result = torch.zeros((output_shape[0], channels, output_shape[2], output_shape[3]), 
                                   device=device, dtype=sample_result.dtype)
                print(f"ğŸ“Š åˆ›å»ºç»“æœå¼ é‡: {result.shape}, å†…å­˜å ç”¨: {result.numel() * 4 / (1024**3):.2f}GB")
            
            # å¤„ç†å½“å‰æ‰¹æ¬¡
            for file_path in batch_files:
                tile_result, bbox = self._load_tile_result(file_path)
                tile_result = tile_result.to(device)
                
                # å°†tileç»“æœå¤åˆ¶åˆ°æœ€ç»ˆç»“æœä¸­
                x1, x2, y1, y2 = bbox
                result[:, :, y1:y2, x1:x2] = tile_result
                
                # ç«‹å³é‡Šæ”¾tileå†…å­˜
                del tile_result
                
                # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                try:
                    os.remove(file_path)
                except:
                    pass
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        
        print(f"âœ… æµå¼å¤„ç†ç»“æœç»„è£…å®Œæˆ")
        return result
    
    @torch.no_grad()
    def vae_tile_forward_streaming(self, z):
        """
        æµå¼VAEå‰å‘ä¼ æ’­ - æ ¸å¿ƒå†…å­˜ä¼˜åŒ–æ–¹æ³•
        
        å…³é”®æ”¹è¿›ï¼š
        1. ä¸é¢„åˆ†é…æ•´ä¸ªç»“æœå¼ é‡
        2. é€ä¸ªå¤„ç†tileï¼Œç«‹å³ä¿å­˜åˆ°ç£ç›˜
        3. æœ€ååˆ†æ‰¹ç»„è£…ç»“æœï¼Œæ§åˆ¶å†…å­˜å³°å€¼
        """
        device = z.device
        dtype = z.dtype
        N, C, H, W = z.shape
        
        # è®¡ç®—è¾“å‡ºå°ºå¯¸
        if self.is_decoder:
            output_height, output_width = H * 8, W * 8
        else:
            output_height, output_width = H // 8, W // 8
        
        # åˆ¤æ–­æ˜¯å¦ä½¿ç”¨æµå¼å¤„ç†
        use_streaming = self._should_use_streaming(output_height, output_width, C)
        
        if not use_streaming:
            print("ğŸ“ ä½¿ç”¨æ ‡å‡†å¤„ç†æ¨¡å¼ï¼ˆå†…å­˜å ç”¨è¾ƒå°ï¼‰")
            return super().vae_tile_forward(z)
        
        print(f"ğŸŒŠ ä½¿ç”¨æµå¼å¤„ç†æ¨¡å¼ - è¾“å…¥: {H}x{W}, è¾“å‡º: {output_height}x{output_width}")
        print(f"ğŸ’¾ é¢„æœŸèŠ‚çœå†…å­˜: {(output_height * output_width * C * 4) / (1024**3):.2f}GB")
        
        # åˆ†å‰²tiles
        tiles, tile_weights, in_bboxes, out_bboxes = self.split_tiles(H, W)
        num_tiles = len(tiles)
        
        print(f"ğŸ”¢ åˆ†å‰²ä¸º{num_tiles}ä¸ªtilesè¿›è¡Œæµå¼å¤„ç†")
        
        # æ„å»ºä»»åŠ¡é˜Ÿåˆ—
        task_queue = self.build_task_queue(self.net, self.is_decoder)
        task_queues = [self.clone_task_queue(task_queue) for _ in range(num_tiles)]
        
        # ä¼°ç®—group normå‚æ•°
        group_norm_param = GroupNormParam()
        if self.color_fix > 0:
            group_norm_param = self.estimate_group_norm(z, task_queue, self.color_fix)
        
        # æµå¼å¤„ç†æ¯ä¸ªtile
        tile_files = []
        processed_tiles = 0
        
        from tqdm import tqdm
        pbar = tqdm(total=num_tiles, desc="ğŸŒŠ æµå¼å¤„ç†tiles")
        
        try:
            for i in range(num_tiles):
                # å¤„ç†å½“å‰tile
                tile = tiles[i].to(device, dtype=dtype)
                current_task_queue = task_queues[i]
                
                # æ‰§è¡Œtileå¤„ç†
                while len(current_task_queue) > 0:
                    task = current_task_queue.pop(0)
                    if task[0] == 'store_res':
                        task[1].append(tile)
                    elif task[0] == 'load_res':
                        tile = task[1].pop()
                    elif task[0] == 'apply_norm':
                        tile = task[1](tile)
                    else:
                        tile = task[1](tile)
                
                # è£å‰ªæœ‰æ•ˆåŒºåŸŸ
                tile_result = crop_valid_region(tile, in_bboxes[i], out_bboxes[i], self.is_decoder)
                
                # ä¿å­˜tileç»“æœåˆ°ä¸´æ—¶æ–‡ä»¶
                temp_file = self._save_tile_result(tile_result, i, out_bboxes[i])
                tile_files.append(temp_file)
                
                # ç«‹å³é‡Šæ”¾å†…å­˜
                del tile, tile_result
                tiles[i] = None  # é‡Šæ”¾åŸå§‹tile
                
                processed_tiles += 1
                pbar.update(1)
                pbar.set_postfix({
                    'processed': f"{processed_tiles}/{num_tiles}",
                    'memory': f"{self._get_memory_usage_gb():.1f}GB"
                })
                
                # å®šæœŸåƒåœ¾å›æ”¶
                if processed_tiles % 4 == 0:
                    gc.collect()
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
        
        finally:
            pbar.close()
        
        # ç»„è£…æœ€ç»ˆç»“æœ
        print(f"ğŸ”§ å¼€å§‹ç»„è£…æœ€ç»ˆç»“æœ...")
        output_shape = (N, C, output_height, output_width)
        result = self._assemble_streaming_result(tile_files, output_shape, device)
        
        print(f"âœ… æµå¼å¤„ç†å®Œæˆï¼Œæœ€ç»ˆå†…å­˜å ç”¨: {self._get_memory_usage_gb():.2f}GB")
        return result
    
    def __call__(self, x):
        """é‡å†™è°ƒç”¨æ–¹æ³•ï¼Œä½¿ç”¨æµå¼å¤„ç†"""
        if self.streaming_mode:
            return self.vae_tile_forward_streaming(x)
        else:
            return super().__call__(x)


class ProgressiveVAEHook(StreamingVAEHook):
    """
    æ¸è¿›å¼VAEå¤„ç†å™¨ - æ”¯æŒå®æ—¶æŸ¥çœ‹å¤„ç†è¿›åº¦
    
    ç‰¹æ€§ï¼š
    1. å®æ—¶ç”Ÿæˆä¸­é—´ç»“æœ
    2. æ”¯æŒè¿›åº¦å›è°ƒ
    3. å¯ä»¥æå‰åœæ­¢å¤„ç†
    4. é€‚åˆäº¤äº’å¼åº”ç”¨
    """
    
    def __init__(self, *args, progress_callback=None, **kwargs):
        super().__init__(*args, **kwargs)
        self.progress_callback = progress_callback
        self.should_stop = False
    
    def stop_processing(self):
        """åœæ­¢å¤„ç†"""
        self.should_stop = True
    
    def _notify_progress(self, processed: int, total: int, intermediate_result=None):
        """é€šçŸ¥å¤„ç†è¿›åº¦"""
        if self.progress_callback:
            progress_info = {
                'processed': processed,
                'total': total,
                'percentage': (processed / total) * 100,
                'intermediate_result': intermediate_result,
                'memory_usage_gb': self._get_memory_usage_gb()
            }
            self.progress_callback(progress_info)
    
    @torch.no_grad()
    def vae_tile_forward_streaming(self, z):
        """æ¸è¿›å¼æµå¼å¤„ç†"""
        # å¦‚æœæœ‰è¿›åº¦å›è°ƒï¼Œå¯ç”¨æ¸è¿›å¼å¤„ç†
        if self.progress_callback:
            return self._progressive_processing(z)
        else:
            return super().vae_tile_forward_streaming(z)
    
    def _progressive_processing(self, z):
        """æ¸è¿›å¼å¤„ç†å®ç°"""
        device = z.device
        dtype = z.dtype
        N, C, H, W = z.shape
        
        # è®¡ç®—è¾“å‡ºå°ºå¯¸
        if self.is_decoder:
            output_height, output_width = H * 8, W * 8
        else:
            output_height, output_width = H // 8, W // 8
        
        # åˆ†å‰²tiles
        tiles, tile_weights, in_bboxes, out_bboxes = self.split_tiles(H, W)
        num_tiles = len(tiles)
        
        # åˆ›å»ºç»“æœå¼ é‡ï¼ˆæ¸è¿›å¼éœ€è¦å®æ—¶æ›´æ–°ï¼‰
        result = torch.zeros((N, C, output_height, output_width), device=device, dtype=dtype)
        
        # æ„å»ºä»»åŠ¡é˜Ÿåˆ—
        task_queue = self.build_task_queue(self.net, self.is_decoder)
        task_queues = [self.clone_task_queue(task_queue) for _ in range(num_tiles)]
        
        # æ¸è¿›å¼å¤„ç†
        for i in range(num_tiles):
            if self.should_stop:
                break
                
            # å¤„ç†å½“å‰tile
            tile = tiles[i].to(device, dtype=dtype)
            current_task_queue = task_queues[i]
            
            # æ‰§è¡Œtileå¤„ç†
            while len(current_task_queue) > 0:
                task = current_task_queue.pop(0)
                if task[0] == 'store_res':
                    task[1].append(tile)
                elif task[0] == 'load_res':
                    tile = task[1].pop()
                elif task[0] == 'apply_norm':
                    tile = task[1](tile)
                else:
                    tile = task[1](tile)
            
            # è£å‰ªæœ‰æ•ˆåŒºåŸŸå¹¶æ›´æ–°ç»“æœ
            tile_result = crop_valid_region(tile, in_bboxes[i], out_bboxes[i], self.is_decoder)
            x1, x2, y1, y2 = out_bboxes[i]
            result[:, :, y1:y2, x1:x2] = tile_result
            
            # é€šçŸ¥è¿›åº¦ï¼ˆä¼ é€’å½“å‰çš„ä¸­é—´ç»“æœï¼‰
            self._notify_progress(i + 1, num_tiles, result.clone())
            
            # æ¸…ç†å†…å­˜
            del tile, tile_result
            tiles[i] = None
        
        return result


def create_streaming_vae_hook(net, tile_size, is_decoder, fast_decoder=True, 
                            fast_encoder=True, color_fix=0, to_gpu=False, 
                            dtype=None, streaming_mode=True, max_memory_gb=4.0,
                            progressive=False, progress_callback=None):
    """
    åˆ›å»ºæµå¼VAEå¤„ç†å™¨çš„å·¥å‚å‡½æ•°
    
    Args:
        streaming_mode: æ˜¯å¦å¯ç”¨æµå¼å¤„ç†
        max_memory_gb: æœ€å¤§å†…å­˜é™åˆ¶ï¼ˆGBï¼‰
        progressive: æ˜¯å¦å¯ç”¨æ¸è¿›å¼å¤„ç†
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
    """
    if progressive:
        return ProgressiveVAEHook(
            net, tile_size, is_decoder, fast_decoder, fast_encoder,
            color_fix, to_gpu, dtype, streaming_mode=streaming_mode,
            max_memory_gb=max_memory_gb, progress_callback=progress_callback
        )
    else:
        return StreamingVAEHook(
            net, tile_size, is_decoder, fast_decoder, fast_encoder,
            color_fix, to_gpu, dtype, streaming_mode=streaming_mode,
            max_memory_gb=max_memory_gb
        )